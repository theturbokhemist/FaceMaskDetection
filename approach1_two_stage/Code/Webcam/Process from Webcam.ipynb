{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5a2e92",
   "metadata": {},
   "source": [
    "# Run Mask Detect on live video\n",
    "\n",
    "\n",
    "## Imports, paths, and info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5869915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76faf701",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "target_h = 112\n",
    "target_w = target_h  # enforce square\n",
    "\n",
    "# detection constants\n",
    "BAD_COLOR = (0, 0, 255)  # red\n",
    "GOOD_COLOR = (0, 128, 0)  # green\n",
    "BOX_LINE_THICKNESS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa663fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_fp = r'C:\\Users\\Andrew\\Documents\\2022 Summer\\Data Mining\\Project\\Webcam\\videos\\surgial mask.mov'\n",
    "#video_fp = r'C:\\Users\\Andrew\\Documents\\2022 Summer\\Data Mining\\Project\\Webcam\\videos\\cloth mask.mov'\n",
    "\n",
    "processed_video_fp = r'C:\\Users\\Andrew\\Documents\\2022 Summer\\Data Mining\\Project\\Webcam\\videos\\processed.avi'\n",
    "\n",
    "full_model_path = r'C:\\Users\\Andrew\\Documents\\2022 Summer\\Data Mining\\Project\\results\\upgrade\\full_model_best'\n",
    "debug_output_dir = r'D:\\data\\face_mask\\webcam\\debug'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532a6f7",
   "metadata": {},
   "source": [
    "## Model and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db980768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: Sequence[int] = (3, 112, 112),\n",
    "        num_classes: int = 2,\n",
    "        channels: Sequence[int] = (8, 16, 32),\n",
    "        kernel_sizes: Sequence[int] = (10, 10, 10, 10),\n",
    "        linear_units: Sequence[int] = (100, 10),\n",
    "        lr: float = 0.001,\n",
    "        epochs: int = 10\n",
    "    ):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.channels = input_size[0:1] + channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.linear_units = linear_units\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.pool = partial(nn.MaxPool2d, kernel_size=2, stride=2)  # first 2 is for 2x2 kernel, second is stride length\n",
    "        self.dropout = nn.Dropout\n",
    "        self.activation = nn.ReLU\n",
    "        self.accuracy = torchmetrics.functional.accuracy\n",
    "        self.conf_matrix = torchmetrics.functional.confusion_matrix\n",
    "        \n",
    "        # optional, define batch norm here\n",
    "        \n",
    "        # build the convolutional layers\n",
    "        conv_layers = list()\n",
    "        for in_channels, out_channels, kernel_size in zip(\n",
    "            self.channels[:-2], self.channels[1:-1], self.kernel_sizes[:-1]\n",
    "        ):\n",
    "            conv_layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    #stride=2,\n",
    "                    #padding='same',\n",
    "                )\n",
    "            )\n",
    "            conv_layers.append(self.activation())\n",
    "            conv_layers.append(self.pool())\n",
    "        # add final layer to convolutions\n",
    "        conv_layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.channels[-2],\n",
    "                out_channels=self.channels[-1],\n",
    "                kernel_size=self.kernel_sizes[-1],\n",
    "                stride=2,\n",
    "                #padding='same',\n",
    "            )\n",
    "        )\n",
    "        conv_layers.append(self.activation())\n",
    "        conv_layers.append(self.pool())\n",
    "\n",
    "        \n",
    "        # turn list into layers\n",
    "        self.conv_net = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        # linear layers\n",
    "        linear_layers = list()\n",
    "        prev_linear_size = self.channels[-1] * 9  # const scale it correctly\n",
    "        for dense_layer_size in self.linear_units:\n",
    "            linear_layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features=prev_linear_size,\n",
    "                    out_features=dense_layer_size,\n",
    "                )\n",
    "            )\n",
    "            linear_layers.append(self.activation())\n",
    "            prev_linear_size=dense_layer_size\n",
    "            \n",
    "        self.penultimate_dense = nn.Sequential(*linear_layers)\n",
    "        self.ultimate_dense = nn.Linear(\n",
    "            in_features=self.linear_units[-1],\n",
    "            out_features=self.num_classes\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_net(x)\n",
    "        x = self.flatten(x)\n",
    "        # may need to expand dense entry since flatten\n",
    "        x = self.penultimate_dense(x)\n",
    "        x = self.ultimate_dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, verbose=False):\n",
    "    #model = model.float()  # sometime fixes random obscure type error\n",
    "    model.train()  # configures for training, grad on, dropout if there is dropout\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute prediction loss\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 5 == 0 and verbose:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return loss\n",
    "\n",
    "# for evaluating on validation data too\n",
    "def test(dataloader, model, loss_fn, verbose=False):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if verbose:\n",
    "        print(f\"Results: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct, test_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444c73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_crop(xl, xr, yt, yb, w, h):\n",
    "    if yt < 0:\n",
    "        diff = abs(yt)\n",
    "        yt = 0\n",
    "        expand_left = int(diff / 2)\n",
    "        expand_right = diff - expand_left\n",
    "        xl = xl - expand_left\n",
    "        xr = xr + expand_right\n",
    "    if xl < 0:\n",
    "        diff = abs(xl)\n",
    "        xl = 0\n",
    "        expand_down = int(diff / 2)\n",
    "        expand_up = diff - expand_down\n",
    "        yb = yb + expand_down\n",
    "        yt = yt - expand_up\n",
    "    if yb > h:\n",
    "        diff = yb - h\n",
    "        yb = h\n",
    "        expand_left = int(diff / 2)\n",
    "        expand_right = diff - expand_left\n",
    "        xl = xl - expand_left\n",
    "        xr = xr + expand_right\n",
    "    if xr > w:\n",
    "        diff = xr - w\n",
    "        xr = w\n",
    "        expand_down = int(diff / 2)\n",
    "        expand_up = diff - expand_down\n",
    "        yb = yb + expand_down\n",
    "        yt = yt - expand_up\n",
    "    if yt < 0 or xl < 0 or yb > h or xr > w:\n",
    "        print('coords error after correction')\n",
    "    return xl, xr, yt, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f9f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_square_expansion(xl, xr, yt, yb, w, h):\n",
    "    bbh = yb - yt\n",
    "    bbw = xr - xl\n",
    "    if bbh > bbw:\n",
    "        diff = bbh - bbw\n",
    "        expand_left = int(diff/2)\n",
    "        expand_right = diff - expand_left\n",
    "        xl = xl - expand_left\n",
    "        xr = xr + expand_right\n",
    "    elif bbw > bbh:\n",
    "        diff = bbw - bbh\n",
    "        expand_down = int(diff/2)\n",
    "        expand_up = diff - expand_down\n",
    "        yb = yb + expand_down\n",
    "        yt = yt - expand_up\n",
    "    \n",
    "    return xl, xr, yt, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b0f1dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# for debug\n",
    "counter = 0\n",
    "\n",
    "# classification smoothing\n",
    "classification = 0.5  # init to no confidence in either direction\n",
    "smoothing_adaptability = .1  # 1.0 means display most recent detection,\n",
    "    # 0.0 means do not update classification at all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load classification model\n",
    "model = torch.load(full_model_path)\n",
    "model.eval()\n",
    "\n",
    "#cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(video_fp)\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "size = (frame_width, frame_height)\n",
    "\n",
    "output_vid_writer = cv2.VideoWriter(processed_video_fp, \n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         10, size)\n",
    "\n",
    "output_vid_writer = cv2.VideoWriter(processed_video_fp, \n",
    "                         cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "                         10, size)\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "      while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            break\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "\n",
    "        # Draw the face detection annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                \n",
    "                # get height, width and depth of image frame\n",
    "                h, w, d = image.shape\n",
    "                \n",
    "                # draw face\n",
    "                #mp_drawing.draw_detection(image, detection)\n",
    "                \n",
    "                # get bounding box and transform normalized coords to pixel coords\n",
    "                rbb = detection.location_data.relative_bounding_box\n",
    "                rect_start_point = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                    rbb.xmin, rbb.ymin, w, h)\n",
    "                rect_end_point = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                    rbb.xmin + rbb.width, rbb.ymin + rbb.height, w, h)\n",
    "                \n",
    "                if rect_start_point is not None and rect_end_point is not None:\n",
    "                    # get individual coordinates from the tuples and create the square\n",
    "                    xl, yt = rect_start_point\n",
    "                    xr, yb = rect_end_point\n",
    "                    xl, xr, yt, yb = rect_square_expansion(xl, xr, yt, yb, w, h)\n",
    "                    \n",
    "                    # expand if nessisary\n",
    "                    expansion = .125\n",
    "                    bbh = yb - yt\n",
    "                    bbw = xr - xl\n",
    "                    amt_to_add = int(expansion * max(bbh, bbw)) \n",
    "                    yt = yt - amt_to_add\n",
    "                    yb = yb + amt_to_add\n",
    "                    xl = xl - amt_to_add\n",
    "                    xr = xr + amt_to_add\n",
    "                    \n",
    "                    xl, xr, yt, yb = correct_crop(xl, xr, yt, yb, w, h)\n",
    "                    xl, xr, yt, yb = correct_crop(xl, xr, yt, yb, w, h)\n",
    "                    \n",
    "                    # crop frame to face\n",
    "                    pil_img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "                    # PIL crop format:  left, top, right, bottom\n",
    "                    crop = [xl, yt, xr, yb]\n",
    "                    pil_crop = pil_img.crop(crop)\n",
    "                    \n",
    "                    # resize\n",
    "                    pil_crop = pil_crop.resize((target_h, target_w), resample=PIL.Image.Resampling.HAMMING)\n",
    "                    \n",
    "                    # debug\n",
    "                    #out_path = os.path.join(debug_output_dir, '{}.png'.format(counter))\n",
    "                    #pil_crop.save(out_path)\n",
    "                    \n",
    "                    # turn image into array\n",
    "                    im_arr = np.array(pil_crop)\n",
    "                    im_arr = im_arr.reshape((3, 112, 112))\n",
    "                    im_arr = im_arr.reshape((1, 3, 112, 112))\n",
    "                    \n",
    "                    # norm\n",
    "                    im_arr = im_arr / 255\n",
    "                    \n",
    "                    model_input = torch.Tensor(im_arr)\n",
    "                    raw_pred = model(model_input.float())  # need to add .float()\n",
    "                    mask_class = raw_pred.argmax(1).item()\n",
    "                    #print(mask_class)\n",
    "                    \n",
    "                    rect_start_point_classification = (xl, yt)\n",
    "                    rect_end_point_classification = (xr, yb)\n",
    "                    \n",
    "                    # smooth out detection\n",
    "                    classification = smoothing_adaptability * mask_class + (1-smoothing_adaptability) * classification\n",
    "                    \n",
    "                    if classification < 0.5:  # no mask/incorrect mask\n",
    "                        cv2.rectangle(image, rect_start_point_classification, rect_end_point_classification,\n",
    "                                      BAD_COLOR, BOX_LINE_THICKNESS)\n",
    "                    else:\n",
    "                        cv2.rectangle(image, rect_start_point_classification, rect_end_point_classification,\n",
    "                                      GOOD_COLOR, BOX_LINE_THICKNESS)\n",
    "\n",
    "                    \n",
    "                    \n",
    "        counter += 1\n",
    "        \n",
    "        \n",
    "        output_vid_writer.write(image)    \n",
    "    \n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # if wait 5 miliseconds and 0xFF == 00011011 (always false)??\n",
    "            break\n",
    "cap.release()\n",
    "output_vid_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06ee2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap.release()\n",
    "output_vid_writer.release()\n",
    "    \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c986cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
